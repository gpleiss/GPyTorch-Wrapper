{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "Below is the example for a Gaussian Process classification example using GpyTorch :class:`.VariationalGaussianProcessClassifier`\n",
    "\n",
    "This example shows how to use a GridInducingVariationalGP module. This classification module is designed for when the inputs of the function you're modeling are one-dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpytorch and gpwrapper in a directory above\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(7)\n",
    "# The train data points are spaced every 1/25 between 0 and 1 inclusive\n",
    "train_x = Variable(torch.linspace(0, 1, 26))\n",
    "# Use the sign function (-1 if value <0, 1 if value>0) to assign\n",
    "# periodic labels to the data\n",
    "train_y = Variable(torch.sign(torch.cos(train_x.data * (8 * math.pi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import BernoulliLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable\n",
    "from gpwrapper import VariationalGaussianProcessClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the GP model\n",
    "# Create a model to classify, we use a GridInducingVariationalGP which exploits\n",
    "# grid structure (the x data points are linspace)\n",
    "# to get fast predictive distributions\n",
    "class GPClassificationModel(gpytorch.models.GridInducingVariationalGP):\n",
    "    def __init__(self):\n",
    "        # For Pipeline to work, we should changed grid_bounds (to (-2,2), for example) \n",
    "        # in the original GPyTorch Example, and increase max_epoches as well !!!!!\n",
    "        super(GPClassificationModel, self).__init__(grid_size=32, grid_bounds=[(0, 1)])\n",
    "        # Near-zero constant mean\n",
    "        self.mean_module = ConstantMean(constant_bounds=[-1e-5,1e-5])\n",
    "        # RBF kernel as universal approximator\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-5, 6))\n",
    "        # Register RBF lengthscale as hyperparameter\n",
    "        self.register_parameter('log_outputscale', nn.Parameter(torch.Tensor([0])), bounds=(-5,6))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Calc predictive mean (zero)\n",
    "        mean_x = self.mean_module(x)\n",
    "        # Calc predictive covariance\n",
    "        covar_x = self.covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        # Make predictive distribution from predictive mean and covariance\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        return latent_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m665.2292\u001b[0m  0.0260\n",
      "      2      \u001b[36m473.0207\u001b[0m  0.0453\n",
      "      3      \u001b[36m338.7597\u001b[0m  0.0426\n",
      "      4      \u001b[36m253.0622\u001b[0m  0.0506\n",
      "      5      \u001b[36m188.2359\u001b[0m  0.0460\n",
      "      6      \u001b[36m132.4029\u001b[0m  0.0427\n",
      "      7       \u001b[36m87.9939\u001b[0m  0.0467\n",
      "      8       \u001b[36m57.8081\u001b[0m  0.0495\n",
      "      9       \u001b[36m40.1763\u001b[0m  0.0472\n",
      "     10       \u001b[36m30.2989\u001b[0m  0.0428\n",
      "     11       \u001b[36m24.1678\u001b[0m  0.0410\n",
      "     12       \u001b[36m19.4501\u001b[0m  0.0444\n",
      "     13       \u001b[36m17.4781\u001b[0m  0.0427\n",
      "     14       \u001b[36m17.0752\u001b[0m  0.0446\n",
      "     15       \u001b[36m17.0454\u001b[0m  0.0401\n",
      "     16       17.1284  0.0419\n",
      "     17       \u001b[36m16.0213\u001b[0m  0.0419\n",
      "     18       \u001b[36m14.7203\u001b[0m  0.0408\n",
      "     19       \u001b[36m14.3037\u001b[0m  0.0758\n",
      "     20       \u001b[36m13.1166\u001b[0m  0.0944\n",
      "     21       \u001b[36m11.9015\u001b[0m  0.0612\n",
      "     22       \u001b[36m10.4692\u001b[0m  0.0449\n",
      "     23        \u001b[36m9.3376\u001b[0m  0.0505\n",
      "     24        \u001b[36m8.5778\u001b[0m  0.0449\n",
      "     25        \u001b[36m7.2836\u001b[0m  0.0425\n",
      "     26        \u001b[36m6.1544\u001b[0m  0.0421\n",
      "     27        \u001b[36m5.0163\u001b[0m  0.0456\n",
      "     28        \u001b[36m4.7313\u001b[0m  0.0500\n",
      "     29        \u001b[36m4.1420\u001b[0m  0.0440\n",
      "     30        \u001b[36m3.7540\u001b[0m  0.0456\n",
      "     31        \u001b[36m3.1477\u001b[0m  0.0415\n",
      "     32        \u001b[36m3.0540\u001b[0m  0.0544\n",
      "     33        3.1183  0.0553\n",
      "     34        3.1926  0.0528\n",
      "     35        3.4240  0.0512\n",
      "     36        \u001b[36m2.8996\u001b[0m  0.0500\n",
      "     37        2.9407  0.0435\n",
      "     38        3.2007  0.0438\n",
      "     39        3.8495  0.0433\n",
      "     40        3.0241  0.0410\n",
      "     41        3.1448  0.0425\n",
      "     42        2.9569  0.0429\n",
      "     43        3.4101  0.0460\n",
      "     44        3.5538  0.0435\n",
      "     45        3.3393  0.0428\n",
      "     46        3.6240  0.0500\n",
      "     47        2.9035  0.0472\n",
      "     48        3.2106  0.0485\n",
      "     49        3.3724  0.0428\n",
      "     50        3.9879  0.0431\n",
      "     51        3.2045  0.0419\n",
      "     52        3.3635  0.0446\n",
      "     53        3.4373  0.0976\n",
      "     54        4.0207  0.0940\n",
      "     55        3.2082  0.0856\n",
      "     56        3.1590  0.0459\n",
      "     57        3.1992  0.0445\n",
      "     58        \u001b[36m2.8145\u001b[0m  0.0413\n",
      "     59        3.0764  0.0420\n",
      "     60        3.3444  0.0430\n",
      "     61        2.9973  0.0429\n",
      "     62        3.1308  0.0471\n",
      "     63        \u001b[36m1.9722\u001b[0m  0.0904\n",
      "     64        2.7226  0.0749\n",
      "     65        2.9604  0.0893\n",
      "     66        2.9417  0.0553\n",
      "     67        2.5192  0.0468\n",
      "     68        2.5546  0.0484\n",
      "     69        2.9352  0.0942\n",
      "     70        2.9845  0.1023\n",
      "     71        2.5528  0.0869\n",
      "     72        2.4266  0.1050\n",
      "     73        2.6458  0.0553\n",
      "     74        2.3642  0.0896\n",
      "     75        2.3372  0.0933\n",
      "     76        2.2415  0.0777\n",
      "     77        2.3994  0.0617\n",
      "     78        2.3982  0.0467\n",
      "     79        2.6748  0.0556\n",
      "     80        \u001b[36m1.9707\u001b[0m  0.0934\n",
      "     81        2.0946  0.1540\n",
      "     82        2.8930  0.1278\n",
      "     83        \u001b[36m1.9318\u001b[0m  0.1571\n",
      "     84        2.2772  0.0622\n",
      "     85        2.0831  0.0686\n",
      "     86        1.9952  0.0596\n",
      "     87        2.6971  0.0569\n",
      "     88        2.2230  0.0801\n",
      "     89        2.3772  0.0941\n",
      "     90        2.6269  0.0736\n",
      "     91        2.8545  0.0504\n",
      "     92        2.5836  0.0424\n",
      "     93        2.1359  0.0413\n",
      "     94        2.1138  0.0425\n",
      "     95        2.0591  0.0429\n",
      "     96        2.2176  0.0420\n",
      "     97        1.9657  0.0421\n",
      "     98        2.4608  0.0852\n",
      "     99        2.4740  0.0707\n",
      "    100        2.2024  0.0758\n",
      "    101        2.1726  0.0459\n",
      "    102        2.6518  0.0459\n",
      "    103        2.1138  0.0440\n",
      "    104        2.3793  0.0425\n",
      "    105        1.9832  0.0565\n",
      "    106        2.1464  0.0391\n",
      "    107        2.0232  0.0401\n",
      "    108        2.7048  0.0721\n",
      "    109        2.4587  0.0784\n",
      "    110        2.1266  0.0463\n",
      "    111        2.6222  0.0432\n",
      "    112        2.0253  0.0409\n",
      "    113        2.1000  0.0370\n",
      "    114        2.4347  0.0361\n",
      "    115        1.9482  0.0389\n",
      "    116        2.4602  0.0353\n",
      "    117        2.0551  0.0410\n",
      "    118        1.9721  0.1125\n",
      "    119        \u001b[36m1.7916\u001b[0m  0.1250\n",
      "    120        2.0633  0.1227\n",
      "    121        \u001b[36m1.6845\u001b[0m  0.0617\n",
      "    122        \u001b[36m1.6834\u001b[0m  0.0375\n",
      "    123        2.2658  0.0442\n",
      "    124        2.1374  0.0581\n",
      "    125        2.0093  0.0476\n",
      "    126        2.0618  0.0455\n",
      "    127        2.0056  0.0389\n",
      "    128        1.9369  0.0572\n",
      "    129        1.8398  0.1326\n",
      "    130        2.3742  0.1112\n",
      "    131        1.9085  0.0450\n",
      "    132        2.1683  0.0435\n",
      "    133        1.9840  0.0388\n",
      "    134        1.9061  0.0393\n",
      "    135        1.7726  0.0418\n",
      "    136        1.7031  0.0466\n",
      "    137        1.7701  0.0386\n",
      "    138        \u001b[36m1.6797\u001b[0m  0.0389\n",
      "    139        1.9576  0.0378\n",
      "    140        1.8355  0.1220\n",
      "    141        2.0628  0.1064\n",
      "    142        1.8810  0.1338\n",
      "    143        1.8121  0.0607\n",
      "    144        2.2985  0.0412\n",
      "    145        1.7439  0.0403\n",
      "    146        1.8859  0.0438\n",
      "    147        1.8095  0.0377\n",
      "    148        1.8728  0.0409\n",
      "    149        2.0246  0.0468\n",
      "    150        1.7245  0.0492\n",
      "    151        \u001b[36m1.4178\u001b[0m  0.0546\n",
      "    152        1.5712  0.0423\n",
      "    153        1.7656  0.0465\n",
      "    154        2.0962  0.0428\n",
      "    155        1.7609  0.0453\n",
      "    156        1.9885  0.0497\n",
      "    157        2.2187  0.0515\n",
      "    158        1.7184  0.1061\n",
      "    159        1.9179  0.1492\n",
      "    160        1.8295  0.0540\n",
      "    161        2.1660  0.0877\n",
      "    162        1.8419  0.0800\n",
      "    163        \u001b[36m1.2541\u001b[0m  0.0515\n",
      "    164        1.8195  0.0582\n",
      "    165        2.2178  0.0964\n",
      "    166        1.8668  0.0907\n",
      "    167        1.9669  0.0678\n",
      "    168        1.5243  0.1182\n",
      "    169        1.5725  0.1176\n",
      "    170        \u001b[36m1.1451\u001b[0m  0.1268\n",
      "    171        1.9209  0.0696\n",
      "    172        1.3900  0.0521\n",
      "    173        1.4907  0.0544\n",
      "    174        1.8457  0.0502\n",
      "    175        1.6915  0.0489\n",
      "    176        1.6073  0.0437\n",
      "    177        2.0026  0.0447\n",
      "    178        1.7224  0.0530\n",
      "    179        1.6784  0.0881\n",
      "    180        1.5148  0.0777\n",
      "    181        1.6400  0.0594\n",
      "    182        1.6080  0.0551\n",
      "    183        1.8006  0.0515\n",
      "    184        1.5351  0.0460\n",
      "    185        1.5064  0.0855\n",
      "    186        1.2262  0.1032\n",
      "    187        1.9557  0.0859\n",
      "    188        \u001b[36m1.1362\u001b[0m  0.0861\n",
      "    189        1.3633  0.0834\n",
      "    190        1.7638  0.0665\n",
      "    191        2.1945  0.0468\n",
      "    192        2.1422  0.0471\n",
      "    193        1.5288  0.1020\n",
      "    194        1.4875  0.0830\n",
      "    195        1.8419  0.0464\n",
      "    196        1.4508  0.0462\n",
      "    197        1.6581  0.0967\n",
      "    198        1.7607  0.1099\n",
      "    199        1.1924  0.0808\n",
      "    200        1.2829  0.0821\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Wrap the model into our GP Wrapper\n",
    "GPWrapper = VariationalGaussianProcessClassifier(\n",
    "    module = GPClassificationModel,\n",
    "    train_split = None,\n",
    "    max_epochs = 200\n",
    ")\n",
    "\n",
    "# Step 3: Find optimal model hyperparameters\n",
    "GPWrapper.fit(X=train_x, y=train_y)\n",
    "\n",
    "# Step 4: Prediction\n",
    "# Test x are regularly spaced by 0.01 0,1 inclusive\n",
    "test_x = Variable(torch.linspace(0, 1, 101))\n",
    "observed_pred = GPWrapper.predict_proba(X=test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (*) Step 5: Plotting\n",
    "# Initialize fig and axes for plot\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "# Plotting function\n",
    "# A lot of this should be consolidated as helper between different notebooks\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    ax.plot(train_x.data.numpy(), train_y.data.numpy(), 'k*')\n",
    "    pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1)\n",
    "    ax.plot(test_x.data.numpy(), pred_labels.data.numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Plot square wave predictions\n",
    "ax_plot(observed_ax, observed_pred, 'Observed Values (Likelihood)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sklearn Pipeline\n",
    "Same as skorch, our wrapper provides an sklearn-compatible interface, so it is possible to put it into an sklearn Pipeline. An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m526.7797\u001b[0m  0.0316\n",
      "      2      \u001b[36m354.3727\u001b[0m  0.0444\n",
      "      3      \u001b[36m232.0656\u001b[0m  0.0384\n",
      "      4      \u001b[36m156.2462\u001b[0m  0.0390\n",
      "      5      \u001b[36m101.7927\u001b[0m  0.0378\n",
      "      6       \u001b[36m59.7346\u001b[0m  0.0398\n",
      "      7       \u001b[36m32.4055\u001b[0m  0.0460\n",
      "      8       \u001b[36m19.2975\u001b[0m  0.0463\n",
      "      9       \u001b[36m15.6292\u001b[0m  0.1423\n",
      "     10       \u001b[36m15.2891\u001b[0m  0.0906\n",
      "     11       \u001b[36m14.9444\u001b[0m  0.0446\n",
      "     12       \u001b[36m13.4122\u001b[0m  0.0485\n",
      "     13       \u001b[36m11.3031\u001b[0m  0.0450\n",
      "     14        \u001b[36m9.8106\u001b[0m  0.0485\n",
      "     15        \u001b[36m8.5520\u001b[0m  0.0465\n",
      "     16        \u001b[36m8.2542\u001b[0m  0.0431\n",
      "     17        \u001b[36m7.2996\u001b[0m  0.0442\n",
      "     18        \u001b[36m7.0582\u001b[0m  0.0465\n",
      "     19        \u001b[36m6.4610\u001b[0m  0.0464\n",
      "     20        \u001b[36m6.0982\u001b[0m  0.0395\n",
      "     21        \u001b[36m5.9499\u001b[0m  0.0495\n",
      "     22        \u001b[36m5.3710\u001b[0m  0.0527\n",
      "     23        5.4890  0.0494\n",
      "     24        \u001b[36m4.8685\u001b[0m  0.0529\n",
      "     25        5.4636  0.0506\n",
      "     26        4.8729  0.0462\n",
      "     27        \u001b[36m4.5530\u001b[0m  0.0506\n",
      "     28        4.8972  0.0425\n",
      "     29        4.7505  0.0450\n",
      "     30        5.4522  0.0480\n",
      "     31        4.7942  0.0444\n",
      "     32        \u001b[36m4.4945\u001b[0m  0.0445\n",
      "     33        4.7528  0.0448\n",
      "     34        4.5607  0.0408\n",
      "     35        4.7248  0.0444\n",
      "     36        5.0849  0.0428\n",
      "     37        5.2945  0.0447\n",
      "     38        4.7381  0.0418\n",
      "     39        \u001b[36m4.4629\u001b[0m  0.0388\n",
      "     40        4.8688  0.0451\n",
      "     41        4.8320  0.0431\n",
      "     42        4.5879  0.0443\n",
      "     43        4.8929  0.0411\n",
      "     44        \u001b[36m4.4144\u001b[0m  0.0430\n",
      "     45        4.6122  0.0456\n",
      "     46        \u001b[36m4.3982\u001b[0m  0.0438\n",
      "     47        \u001b[36m4.2860\u001b[0m  0.0449\n",
      "     48        4.5720  0.0429\n",
      "     49        4.5810  0.0393\n",
      "     50        \u001b[36m4.1136\u001b[0m  0.0427\n",
      "     51        4.1496  0.0413\n",
      "     52        4.8233  0.0431\n",
      "     53        4.8716  0.0396\n",
      "     54        4.2211  0.0428\n",
      "     55        4.2568  0.0381\n",
      "     56        4.1800  0.0446\n",
      "     57        4.6293  0.0831\n",
      "     58        4.3250  0.0712\n",
      "     59        4.5265  0.0620\n",
      "     60        4.2335  0.0460\n",
      "     61        4.2884  0.0416\n",
      "     62        \u001b[36m3.9988\u001b[0m  0.0454\n",
      "     63        4.4946  0.0430\n",
      "     64        4.1043  0.0416\n",
      "     65        4.1079  0.0418\n",
      "     66        4.4101  0.0420\n",
      "     67        4.4327  0.0462\n",
      "     68        4.0511  0.0436\n",
      "     69        4.4050  0.0426\n",
      "     70        \u001b[36m3.8288\u001b[0m  0.0505\n",
      "     71        \u001b[36m3.6397\u001b[0m  0.0505\n",
      "     72        3.7962  0.0523\n",
      "     73        3.7727  0.0506\n",
      "     74        3.8985  0.0507\n",
      "     75        3.7985  0.0470\n",
      "     76        \u001b[36m3.3795\u001b[0m  0.0426\n",
      "     77        3.6668  0.0438\n",
      "     78        3.6072  0.0427\n",
      "     79        3.4700  0.0411\n",
      "     80        3.6271  0.0435\n",
      "     81        3.3842  0.0442\n",
      "     82        3.5947  0.0413\n",
      "     83        3.7823  0.0397\n",
      "     84        3.7550  0.0437\n",
      "     85        4.0211  0.0395\n",
      "     86        3.7624  0.0412\n",
      "     87        3.7032  0.0427\n",
      "     88        \u001b[36m3.2392\u001b[0m  0.0416\n",
      "     89        3.4446  0.0428\n",
      "     90        3.3494  0.0418\n",
      "     91        3.6573  0.0906\n",
      "     92        3.3548  0.1131\n",
      "     93        3.3671  0.0513\n",
      "     94        3.3717  0.0469\n",
      "     95        \u001b[36m3.1611\u001b[0m  0.0470\n",
      "     96        \u001b[36m3.0643\u001b[0m  0.0450\n",
      "     97        3.7988  0.1199\n",
      "     98        3.2259  0.0860\n",
      "     99        3.1431  0.0513\n",
      "    100        3.3810  0.0509\n",
      "    101        \u001b[36m2.8699\u001b[0m  0.0698\n",
      "    102        3.3491  0.0607\n",
      "    103        3.3063  0.0639\n",
      "    104        3.1285  0.0706\n",
      "    105        3.2606  0.0785\n",
      "    106        3.2738  0.0559\n",
      "    107        3.6084  0.0605\n",
      "    108        2.9910  0.0536\n",
      "    109        3.2326  0.0520\n",
      "    110        3.0994  0.0503\n",
      "    111        3.4249  0.0465\n",
      "    112        2.9196  0.0444\n",
      "    113        3.0249  0.1044\n",
      "    114        2.9037  0.1442\n",
      "    115        2.8899  0.0582\n",
      "    116        3.0897  0.0563\n",
      "    117        3.0164  0.1367\n",
      "    118        \u001b[36m2.8654\u001b[0m  0.2333\n",
      "    119        \u001b[36m2.8148\u001b[0m  0.1470\n",
      "    120        3.0671  0.0973\n",
      "    121        2.9832  0.0689\n",
      "    122        2.8696  0.0568\n",
      "    123        2.8558  0.0522\n",
      "    124        \u001b[36m2.6761\u001b[0m  0.0410\n",
      "    125        2.6856  0.0393\n",
      "    126        \u001b[36m2.6674\u001b[0m  0.0443\n",
      "    127        2.8691  0.0528\n",
      "    128        2.6975  0.0535\n",
      "    129        \u001b[36m2.5538\u001b[0m  0.0553\n",
      "    130        2.8381  0.0450\n",
      "    131        2.6599  0.0427\n",
      "    132        2.8373  0.0443\n",
      "    133        2.5697  0.1203\n",
      "    134        2.7995  0.0711\n",
      "    135        2.7627  0.1290\n",
      "    136        3.0390  0.1015\n",
      "    137        2.7397  0.1025\n",
      "    138        \u001b[36m2.5129\u001b[0m  0.0454\n",
      "    139        2.5660  0.0415\n",
      "    140        2.7208  0.0432\n",
      "    141        2.6951  0.0424\n",
      "    142        2.5418  0.0398\n",
      "    143        2.7881  0.0375\n",
      "    144        2.6059  0.0497\n",
      "    145        2.7110  0.0914\n",
      "    146        2.5684  0.0947\n",
      "    147        \u001b[36m2.4488\u001b[0m  0.0837\n",
      "    148        \u001b[36m2.2932\u001b[0m  0.0851\n",
      "    149        2.6611  0.1027\n",
      "    150        2.7019  0.0611\n",
      "    151        2.5396  0.0403\n",
      "    152        2.6170  0.0424\n",
      "    153        2.4430  0.0408\n",
      "    154        2.4826  0.0414\n",
      "    155        2.4744  0.0392\n",
      "    156        2.4984  0.0415\n",
      "    157        \u001b[36m2.1944\u001b[0m  0.0463\n",
      "    158        2.3006  0.0402\n",
      "    159        2.3450  0.0386\n",
      "    160        2.5691  0.0372\n",
      "    161        2.3077  0.0456\n",
      "    162        2.4521  0.0479\n",
      "    163        2.3907  0.0485\n",
      "    164        2.3282  0.0362\n",
      "    165        2.5540  0.0375\n",
      "    166        2.1996  0.0368\n",
      "    167        2.4748  0.0366\n",
      "    168        2.4042  0.0391\n",
      "    169        2.4446  0.0425\n",
      "    170        \u001b[36m2.1459\u001b[0m  0.0395\n",
      "    171        2.2073  0.0411\n",
      "    172        2.3301  0.0421\n",
      "    173        2.3505  0.0440\n",
      "    174        2.3553  0.0402\n",
      "    175        \u001b[36m2.0509\u001b[0m  0.0379\n",
      "    176        2.1920  0.0413\n",
      "    177        2.4071  0.0449\n",
      "    178        2.1748  0.0431\n",
      "    179        2.3605  0.0390\n",
      "    180        2.2952  0.0409\n",
      "    181        2.1308  0.0681\n",
      "    182        2.2479  0.0867\n",
      "    183        2.3413  0.0699\n",
      "    184        2.3114  0.0459\n",
      "    185        2.1504  0.0458\n",
      "    186        2.0694  0.0513\n",
      "    187        2.1004  0.0488\n",
      "    188        2.0996  0.0468\n",
      "    189        \u001b[36m2.0174\u001b[0m  0.0479\n",
      "    190        2.2054  0.0498\n",
      "    191        2.0449  0.0544\n",
      "    192        2.1593  0.0558\n",
      "    193        2.1406  0.0578\n",
      "    194        2.0547  0.0596\n",
      "    195        2.0251  0.0537\n",
      "    196        2.1546  0.0975\n",
      "    197        2.1128  0.0762\n",
      "    198        2.0627  0.0777\n",
      "    199        2.2213  0.0682\n",
      "    200        2.0613  0.0907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('GP', <class 'gpwrapper.VariationalGaussianProcessClassifier'>[initialized](\n",
       "  module_=GPClassificationModel(\n",
       "    (mean_module): ConstantMean()\n",
       "    (covar_module): GridKernel(\n",
       "      (base_kernel_module): RBFKernel()\n",
       "    )\n",
       "  ),\n",
       "))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Note: For Pipeline to work, we should changed grid_bounds in cell [4](to (-2,2), for example) \n",
    "# in the original GPyTorch Example, and increase max_epoches as well !!!!!\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('GP', GPWrapper),\n",
    "])\n",
    "\n",
    "pipe.fit(X=train_x.unsqueeze(-1), y=train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search (Under construction)\n",
    "Same as skorch, another advantage of our wrapper is that you can perform an sklearn GridSearchCV or RandomizedSearchCV in Gpytorch to find optimal hyperparameters. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m966.8552\u001b[0m  0.0271\n",
      "      2      \u001b[36m925.8406\u001b[0m  0.0425\n",
      "      3      \u001b[36m887.6171\u001b[0m  0.0413\n",
      "      4      \u001b[36m851.6039\u001b[0m  0.0392\n",
      "      5      \u001b[36m818.0740\u001b[0m  0.0354\n",
      "      6      \u001b[36m786.7318\u001b[0m  0.0451\n",
      "      7      \u001b[36m757.1562\u001b[0m  0.0373\n",
      "      8      \u001b[36m729.5147\u001b[0m  0.0355\n",
      "      9      \u001b[36m703.5017\u001b[0m  0.0418\n",
      "     10      \u001b[36m678.7001\u001b[0m  0.0727\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m971.7765\u001b[0m  0.1070\n",
      "      2      \u001b[36m929.5168\u001b[0m  0.0930\n",
      "      3      \u001b[36m889.9679\u001b[0m  0.1153\n",
      "      4      \u001b[36m853.0980\u001b[0m  0.0867\n",
      "      5      \u001b[36m818.7068\u001b[0m  0.0931\n",
      "      6      \u001b[36m786.6728\u001b[0m  0.0885\n",
      "      7      \u001b[36m756.7657\u001b[0m  0.0894\n",
      "      8      \u001b[36m728.2684\u001b[0m  0.0486\n",
      "      9      \u001b[36m701.8292\u001b[0m  0.0481\n",
      "     10      \u001b[36m677.0917\u001b[0m  0.0478\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m937.8278\u001b[0m  0.0454\n",
      "      2      \u001b[36m898.8004\u001b[0m  0.0463\n",
      "      3      \u001b[36m862.3668\u001b[0m  0.0459\n",
      "      4      \u001b[36m828.1891\u001b[0m  0.0440\n",
      "      5      \u001b[36m796.4184\u001b[0m  0.0418\n",
      "      6      \u001b[36m766.6042\u001b[0m  0.0434\n",
      "      7      \u001b[36m738.6562\u001b[0m  0.0424\n",
      "      8      \u001b[36m712.5088\u001b[0m  0.0648\n",
      "      9      \u001b[36m687.8648\u001b[0m  0.0569\n",
      "     10      \u001b[36m664.3586\u001b[0m  0.0529\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m986.2850\u001b[0m  0.0465\n",
      "      2      \u001b[36m945.4832\u001b[0m  0.0411\n",
      "      3      \u001b[36m907.2221\u001b[0m  0.0409\n",
      "      4      \u001b[36m871.3743\u001b[0m  0.0383\n",
      "      5      \u001b[36m837.9456\u001b[0m  0.0393\n",
      "      6      \u001b[36m806.5980\u001b[0m  0.1037\n",
      "      7      \u001b[36m777.2881\u001b[0m  0.0704\n",
      "      8      \u001b[36m749.5272\u001b[0m  0.0449\n",
      "      9      \u001b[36m723.6791\u001b[0m  0.0432\n",
      "     10      \u001b[36m699.2109\u001b[0m  0.0423\n",
      "     11      \u001b[36m676.1063\u001b[0m  0.0433\n",
      "     12      \u001b[36m654.1302\u001b[0m  0.0396\n",
      "     13      \u001b[36m633.1185\u001b[0m  0.0410\n",
      "     14      \u001b[36m612.8626\u001b[0m  0.0413\n",
      "     15      \u001b[36m593.8795\u001b[0m  0.0431\n",
      "     16      \u001b[36m575.3472\u001b[0m  0.0415\n",
      "     17      \u001b[36m557.3588\u001b[0m  0.0451\n",
      "     18      \u001b[36m539.9458\u001b[0m  0.0448\n",
      "     19      \u001b[36m523.4719\u001b[0m  0.0498\n",
      "     20      \u001b[36m507.4638\u001b[0m  0.0464\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m973.3901\u001b[0m  0.0520\n",
      "      2      \u001b[36m932.7216\u001b[0m  0.0485\n",
      "      3      \u001b[36m894.5294\u001b[0m  0.0427\n",
      "      4      \u001b[36m859.1252\u001b[0m  0.0390\n",
      "      5      \u001b[36m825.7062\u001b[0m  0.0382\n",
      "      6      \u001b[36m794.7026\u001b[0m  0.0395\n",
      "      7      \u001b[36m765.5668\u001b[0m  0.1010\n",
      "      8      \u001b[36m738.2819\u001b[0m  0.0772\n",
      "      9      \u001b[36m712.4518\u001b[0m  0.0435\n",
      "     10      \u001b[36m688.3277\u001b[0m  0.0413\n",
      "     11      \u001b[36m665.3566\u001b[0m  0.0472\n",
      "     12      \u001b[36m643.6547\u001b[0m  0.0423\n",
      "     13      \u001b[36m622.9305\u001b[0m  0.0432\n",
      "     14      \u001b[36m602.9006\u001b[0m  0.0387\n",
      "     15      \u001b[36m583.6602\u001b[0m  0.0504\n",
      "     16      \u001b[36m565.3967\u001b[0m  0.0923\n",
      "     17      \u001b[36m547.5464\u001b[0m  0.0937\n",
      "     18      \u001b[36m530.5691\u001b[0m  0.1237\n",
      "     19      \u001b[36m514.0062\u001b[0m  0.0846\n",
      "     20      \u001b[36m497.8770\u001b[0m  0.0998\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m928.9119\u001b[0m  0.0909\n",
      "      2      \u001b[36m891.0208\u001b[0m  0.0575\n",
      "      3      \u001b[36m855.5029\u001b[0m  0.0450\n",
      "      4      \u001b[36m822.1747\u001b[0m  0.0425\n",
      "      5      \u001b[36m791.5403\u001b[0m  0.0426\n",
      "      6      \u001b[36m762.3770\u001b[0m  0.0433\n",
      "      7      \u001b[36m735.2989\u001b[0m  0.0450\n",
      "      8      \u001b[36m709.8485\u001b[0m  0.0424\n",
      "      9      \u001b[36m685.9985\u001b[0m  0.0408\n",
      "     10      \u001b[36m663.4717\u001b[0m  0.0418\n",
      "     11      \u001b[36m642.0095\u001b[0m  0.0440\n",
      "     12      \u001b[36m621.6423\u001b[0m  0.0418\n",
      "     13      \u001b[36m601.9626\u001b[0m  0.0568\n",
      "     14      \u001b[36m583.3490\u001b[0m  0.0500\n",
      "     15      \u001b[36m565.3170\u001b[0m  0.0510\n",
      "     16      \u001b[36m547.9556\u001b[0m  0.1005\n",
      "     17      \u001b[36m531.0473\u001b[0m  0.0894\n",
      "     18      \u001b[36m515.0221\u001b[0m  0.0737\n",
      "     19      \u001b[36m499.1124\u001b[0m  0.0845\n",
      "     20      \u001b[36m483.8721\u001b[0m  0.0745\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m933.3215\u001b[0m  0.0649\n",
      "      2      \u001b[36m854.7814\u001b[0m  0.0693\n",
      "      3      \u001b[36m785.7557\u001b[0m  0.0796\n",
      "      4      \u001b[36m726.0068\u001b[0m  0.0470\n",
      "      5      \u001b[36m673.2827\u001b[0m  0.0603\n",
      "      6      \u001b[36m626.8218\u001b[0m  0.0466\n",
      "      7      \u001b[36m584.9643\u001b[0m  0.0440\n",
      "      8      \u001b[36m547.2477\u001b[0m  0.0465\n",
      "      9      \u001b[36m512.8557\u001b[0m  0.0478\n",
      "     10      \u001b[36m480.8258\u001b[0m  0.0494\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m964.0527\u001b[0m  0.0397\n",
      "      2      \u001b[36m884.7335\u001b[0m  0.0459\n",
      "      3      \u001b[36m814.8244\u001b[0m  0.0507\n",
      "      4      \u001b[36m753.4446\u001b[0m  0.0739\n",
      "      5      \u001b[36m699.5814\u001b[0m  0.0726\n",
      "      6      \u001b[36m651.2448\u001b[0m  0.0846\n",
      "      7      \u001b[36m607.6519\u001b[0m  0.1028\n",
      "      8      \u001b[36m568.0898\u001b[0m  0.1244\n",
      "      9      \u001b[36m531.7772\u001b[0m  0.0871\n",
      "     10      \u001b[36m498.0102\u001b[0m  0.0965\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m930.5617\u001b[0m  0.0559\n",
      "      2      \u001b[36m853.8177\u001b[0m  0.0481\n",
      "      3      \u001b[36m786.0625\u001b[0m  0.0434\n",
      "      4      \u001b[36m726.9435\u001b[0m  0.0458\n",
      "      5      \u001b[36m674.9310\u001b[0m  0.1261\n",
      "      6      \u001b[36m628.4583\u001b[0m  0.1177\n",
      "      7      \u001b[36m586.5099\u001b[0m  0.0505\n",
      "      8      \u001b[36m548.5934\u001b[0m  0.0669\n",
      "      9      \u001b[36m513.6430\u001b[0m  0.0651\n",
      "     10      \u001b[36m481.4139\u001b[0m  0.1305\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m940.7463\u001b[0m  0.2126\n",
      "      2      \u001b[36m863.1086\u001b[0m  0.2468\n",
      "      3      \u001b[36m795.0184\u001b[0m  0.0848\n",
      "      4      \u001b[36m735.2751\u001b[0m  0.0602\n",
      "      5      \u001b[36m682.9856\u001b[0m  0.0709\n",
      "      6      \u001b[36m636.0037\u001b[0m  0.0506\n",
      "      7      \u001b[36m593.6064\u001b[0m  0.0645\n",
      "      8      \u001b[36m555.1136\u001b[0m  0.2076\n",
      "      9      \u001b[36m519.6982\u001b[0m  0.0950\n",
      "     10      \u001b[36m486.8774\u001b[0m  0.1689\n",
      "     11      \u001b[36m456.2498\u001b[0m  0.2062\n",
      "     12      \u001b[36m426.9789\u001b[0m  0.1396\n",
      "     13      \u001b[36m399.5409\u001b[0m  0.1202\n",
      "     14      \u001b[36m373.5638\u001b[0m  0.0931\n",
      "     15      \u001b[36m348.9210\u001b[0m  0.0753\n",
      "     16      \u001b[36m325.6649\u001b[0m  0.0633\n",
      "     17      \u001b[36m303.6116\u001b[0m  0.0596\n",
      "     18      \u001b[36m282.8730\u001b[0m  0.0514\n",
      "     19      \u001b[36m263.4024\u001b[0m  0.0569\n",
      "     20      \u001b[36m244.9743\u001b[0m  0.1731\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m993.5732\u001b[0m  0.1962\n",
      "      2      \u001b[36m913.9824\u001b[0m  0.1009\n",
      "      3      \u001b[36m843.3835\u001b[0m  0.0941\n",
      "      4      \u001b[36m781.4806\u001b[0m  0.0994\n",
      "      5      \u001b[36m726.6883\u001b[0m  0.1021\n",
      "      6      \u001b[36m677.4151\u001b[0m  0.0617\n",
      "      7      \u001b[36m633.0270\u001b[0m  0.0516\n",
      "      8      \u001b[36m592.7158\u001b[0m  0.0505\n",
      "      9      \u001b[36m555.6194\u001b[0m  0.0712\n",
      "     10      \u001b[36m521.6566\u001b[0m  0.0667\n",
      "     11      \u001b[36m489.4687\u001b[0m  0.0645\n",
      "     12      \u001b[36m459.3783\u001b[0m  0.0580\n",
      "     13      \u001b[36m431.2968\u001b[0m  0.0558\n",
      "     14      \u001b[36m404.6520\u001b[0m  0.1160\n",
      "     15      \u001b[36m379.0340\u001b[0m  0.1188\n",
      "     16      \u001b[36m354.9990\u001b[0m  0.1135\n",
      "     17      \u001b[36m332.0217\u001b[0m  0.0684\n",
      "     18      \u001b[36m310.3799\u001b[0m  0.0769\n",
      "     19      \u001b[36m289.6290\u001b[0m  0.0658\n",
      "     20      \u001b[36m270.2430\u001b[0m  0.0735\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m911.6815\u001b[0m  0.0545\n",
      "      2      \u001b[36m837.1761\u001b[0m  0.0559\n",
      "      3      \u001b[36m771.8780\u001b[0m  0.0482\n",
      "      4      \u001b[36m714.9859\u001b[0m  0.0511\n",
      "      5      \u001b[36m665.1660\u001b[0m  0.1429\n",
      "      6      \u001b[36m620.4896\u001b[0m  0.1224\n",
      "      7      \u001b[36m580.3549\u001b[0m  0.0716\n",
      "      8      \u001b[36m543.7766\u001b[0m  0.0595\n",
      "      9      \u001b[36m510.2920\u001b[0m  0.0590\n",
      "     10      \u001b[36m478.9467\u001b[0m  0.0614\n",
      "     11      \u001b[36m449.5426\u001b[0m  0.0631\n",
      "     12      \u001b[36m421.6204\u001b[0m  0.0562\n",
      "     13      \u001b[36m395.1863\u001b[0m  0.0564\n",
      "     14      \u001b[36m370.1399\u001b[0m  0.0561\n",
      "     15      \u001b[36m346.1466\u001b[0m  0.0582\n",
      "     16      \u001b[36m323.4133\u001b[0m  0.0560\n",
      "     17      \u001b[36m301.9735\u001b[0m  0.0620\n",
      "     18      \u001b[36m281.6297\u001b[0m  0.0640\n",
      "     19      \u001b[36m262.4581\u001b[0m  0.1034\n",
      "     20      \u001b[36m244.4432\u001b[0m  0.1103\n",
      "\n",
      " gs.best_score_ = 0.5769230769230769, gs.best_params = {'lr': 0.02, 'max_epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'lr': [0.01, 0.02],\n",
    "    'max_epochs': [10, 20],\n",
    "}\n",
    "\n",
    "GPWrapper = VariationalGaussianProcessClassifier(\n",
    "    module = GPClassificationModel,\n",
    "    train_split = None,\n",
    "    max_epochs = 200\n",
    ")\n",
    "\n",
    "gs = GridSearchCV(GPWrapper, params, refit=False, cv=3, scoring='accuracy',\n",
    "                  return_train_score=False)  # Use a different scoring function maybe?\n",
    "\n",
    "gs.fit(X=train_x, y=train_y)\n",
    "print('\\n gs.best_score_ = {}, gs.best_params = {}'.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Need to comment out **line 157 - 161** of `.../anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py`\n",
    "```\n",
    "if hasattr(X, 'take') and (hasattr(indices, 'dtype') and\n",
    "                           indices.dtype.kind == 'i'):\n",
    "    # This is often substantially faster than X[indices]\n",
    "    return X.take(indices, axis=0)\n",
    "else:\n",
    "```\n",
    "Otherwise an error would occur saying\n",
    "`argument 'index' (position 1) must be Tensor, not numpy.ndarray`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
