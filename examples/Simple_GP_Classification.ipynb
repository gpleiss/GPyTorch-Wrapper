{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "\n",
    "Below is the example for a simple Gaussian Process classification example using GpyTorch :class:`.VariationalGaussianProcessClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gpytorch and gpwrapper in a directory above\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch import nn, optim\n",
    "from gpytorch.kernels import RBFKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.random_variables import GaussianRandomVariable\n",
    "from gpwrapper import VariationalGaussianProcessClassifier\n",
    "\n",
    "torch.manual_seed(7)\n",
    "# Grid points are [0,1] every 1/9\n",
    "train_x = Variable(torch.linspace(0, 1, 10))\n",
    "# Labels are unit wave with period 1/2 centered with positive values @ x=0\n",
    "train_y = Variable(torch.sign(torch.cos(train_x.data * (4 * math.pi))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the GP model\n",
    "# Basic classification model with variational inference\n",
    "class GPClassificationModel(gpytorch.models.VariationalGP):\n",
    "    def __init__(self, train_inputs):\n",
    "        super(GPClassificationModel, self).__init__(train_inputs)\n",
    "        # Only non-zero mean function can be learned\n",
    "        self.mean_module = ConstantMean(constant_bounds=[-1e-5, 1e-5])\n",
    "        # Use universal approximator RBF kernel\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-5, 6))\n",
    "        self.register_parameter('log_outputscale', nn.Parameter(torch.Tensor([0])), bounds=(-5, 6))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get predictive mean and covariance\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        # Scale covariance\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        # Package prediction as GaussianRandomVariable\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        # Return predictions\n",
    "        return latent_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m325.9557\u001b[0m  0.0862\n",
      "      2      \u001b[36m228.1859\u001b[0m  0.0134\n",
      "      3      \u001b[36m147.3188\u001b[0m  0.0173\n",
      "      4       \u001b[36m94.1946\u001b[0m  0.0132\n",
      "      5       \u001b[36m57.9264\u001b[0m  0.0128\n",
      "      6       \u001b[36m33.7167\u001b[0m  0.0110\n",
      "      7       \u001b[36m20.0186\u001b[0m  0.0236\n",
      "      8       \u001b[36m14.3599\u001b[0m  0.0122\n",
      "      9       \u001b[36m12.4063\u001b[0m  0.0153\n",
      "     10       \u001b[36m11.9052\u001b[0m  0.0213\n",
      "     11       \u001b[36m11.1336\u001b[0m  0.0129\n",
      "     12        \u001b[36m9.9352\u001b[0m  0.0137\n",
      "     13        \u001b[36m9.7594\u001b[0m  0.0138\n",
      "     14        \u001b[36m9.2419\u001b[0m  0.0108\n",
      "     15        \u001b[36m7.5268\u001b[0m  0.0132\n",
      "     16        \u001b[36m6.7588\u001b[0m  0.0129\n",
      "     17        \u001b[36m6.4355\u001b[0m  0.0107\n",
      "     18        \u001b[36m5.7118\u001b[0m  0.0139\n",
      "     19        \u001b[36m5.5610\u001b[0m  0.0133\n",
      "     20        \u001b[36m5.1346\u001b[0m  0.0131\n",
      "     21        \u001b[36m5.1171\u001b[0m  0.0115\n",
      "     22        5.4638  0.0164\n",
      "     23        \u001b[36m4.9524\u001b[0m  0.0205\n",
      "     24        5.1271  0.0222\n",
      "     25        5.4291  0.0230\n",
      "     26        \u001b[36m4.9319\u001b[0m  0.0123\n",
      "     27        \u001b[36m4.8011\u001b[0m  0.0172\n",
      "     28        4.9526  0.0175\n",
      "     29        \u001b[36m4.6320\u001b[0m  0.0134\n",
      "     30        5.0012  0.0119\n",
      "     31        5.1621  0.0162\n",
      "     32        5.2501  0.0119\n",
      "     33        4.8657  0.0252\n",
      "     34        4.8530  0.0176\n",
      "     35        5.5720  0.0123\n",
      "     36        5.2970  0.0131\n",
      "     37        5.0451  0.0129\n",
      "     38        5.1084  0.0173\n",
      "     39        \u001b[36m4.4069\u001b[0m  0.0135\n",
      "     40        5.2921  0.0124\n",
      "     41        4.6777  0.0167\n",
      "     42        5.1524  0.0099\n",
      "     43        5.2926  0.0176\n",
      "     44        4.4377  0.0149\n",
      "     45        4.5416  0.0113\n",
      "     46        5.3029  0.0144\n",
      "     47        4.7601  0.0168\n",
      "     48        \u001b[36m4.3619\u001b[0m  0.0179\n",
      "     49        4.8692  0.0126\n",
      "     50        4.4026  0.0241\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Wrap the model into our GP Wrapper\n",
    "GPWrapper = VariationalGaussianProcessClassifier(\n",
    "    module = GPClassificationModel,\n",
    "    train_split = None,\n",
    ")\n",
    "\n",
    "# Step 3: Find optimal model hyperparameters\n",
    "GPWrapper.fit(X=train_x, y=train_y)\n",
    "\n",
    "# Step 4: Prediction\n",
    "# Test x are regularly spaced by 0.01 0,1 inclusive\n",
    "test_x = Variable(torch.linspace(0, 1, 101))\n",
    "observed_pred = GPWrapper.predict_proba(X=test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (*) Step 5: Plotting\n",
    "# Initialize fig and axes for plot\n",
    "f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "\n",
    "# Plotting function\n",
    "# A lot of this should be consolidated as helper between different notebooks\n",
    "def ax_plot(ax, rand_var, title):\n",
    "    ax.plot(train_x.data.numpy(), train_y.data.numpy(), 'k*')\n",
    "    pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1)\n",
    "    ax.plot(test_x.data.numpy(), pred_labels.data.numpy(), 'b')\n",
    "    ax.set_ylim([-3, 3])\n",
    "    ax.legend(['Observed Data', 'Mean', 'Confidence'])\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Plot square wave predictions\n",
    "ax_plot(observed_ax, observed_pred, 'Observed Values (Likelihood)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sklearn Pipeline\n",
    "Same as skorch, our wrapper provides an sklearn-compatible interface, so it is possible to put it into an sklearn Pipeline. An example is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m189.6633\u001b[0m  0.0162\n",
      "      2      \u001b[36m115.8089\u001b[0m  0.0162\n",
      "      3       \u001b[36m60.4891\u001b[0m  0.0145\n",
      "      4       \u001b[36m28.6950\u001b[0m  0.0122\n",
      "      5       \u001b[36m12.6096\u001b[0m  0.0120\n",
      "      6        \u001b[36m6.3260\u001b[0m  0.0127\n",
      "      7        \u001b[36m4.7166\u001b[0m  0.0127\n",
      "      8        4.9294  0.0121\n",
      "      9        4.8688  0.0126\n",
      "     10        \u001b[36m4.3113\u001b[0m  0.0105\n",
      "     11        4.3976  0.0111\n",
      "     12        \u001b[36m4.0188\u001b[0m  0.0156\n",
      "     13        4.3445  0.0134\n",
      "     14        4.1298  0.0130\n",
      "     15        4.5064  0.0120\n",
      "     16        4.3699  0.0116\n",
      "     17        4.8133  0.0124\n",
      "     18        4.5483  0.0112\n",
      "     19        4.9878  0.0101\n",
      "     20        5.3043  0.0126\n",
      "     21        5.3614  0.0168\n",
      "     22        5.2772  0.0139\n",
      "     23        5.4562  0.0141\n",
      "     24        5.6869  0.0119\n",
      "     25        5.6429  0.0138\n",
      "     26        5.6271  0.0136\n",
      "     27        5.8642  0.0138\n",
      "     28        5.2855  0.0097\n",
      "     29        5.1485  0.0108\n",
      "     30        5.3748  0.0095\n",
      "     31        5.1599  0.0143\n",
      "     32        4.9541  0.0140\n",
      "     33        5.1553  0.0155\n",
      "     34        5.0062  0.0189\n",
      "     35        4.7072  0.0285\n",
      "     36        5.0861  0.0175\n",
      "     37        4.9210  0.0199\n",
      "     38        4.8896  0.0173\n",
      "     39        4.6769  0.0159\n",
      "     40        4.7278  0.0136\n",
      "     41        4.6273  0.0144\n",
      "     42        4.3644  0.0176\n",
      "     43        4.2833  0.0116\n",
      "     44        4.1785  0.0102\n",
      "     45        4.3466  0.0253\n",
      "     46        4.4340  0.0191\n",
      "     47        4.1429  0.0151\n",
      "     48        4.2509  0.0162\n",
      "     49        4.6228  0.0142\n",
      "     50        4.1628  0.0140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('GP', <class 'gpwrapper.VariationalGaussianProcessClassifier'>[initialized](\n",
       "  module_=GPClassificationModel(\n",
       "    (mean_module): ConstantMean()\n",
       "    (covar_module): RBFKernel()\n",
       "  ),\n",
       "))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('GP', GPWrapper),\n",
    "])\n",
    "\n",
    "pipe.fit(X=train_x.unsqueeze(-1), y=train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search\n",
    "Same as skorch, another advantage of our wrapper is that you can perform an sklearn GridSearchCV or RandomizedSearchCV in Gpytorch to find optimal hyperparameters. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.5632\u001b[0m  0.0104\n",
      "      2      \u001b[36m259.7476\u001b[0m  0.0184\n",
      "      3      \u001b[36m249.9569\u001b[0m  0.0121\n",
      "      4      \u001b[36m240.5389\u001b[0m  0.0104\n",
      "      5      \u001b[36m231.6512\u001b[0m  0.0084\n",
      "      6      \u001b[36m222.8682\u001b[0m  0.0086\n",
      "      7      \u001b[36m214.3439\u001b[0m  0.0100\n",
      "      8      \u001b[36m206.0892\u001b[0m  0.0079\n",
      "      9      \u001b[36m198.2059\u001b[0m  0.0096\n",
      "     10      \u001b[36m190.5099\u001b[0m  0.0093\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m258.7633\u001b[0m  0.0082\n",
      "      2      \u001b[36m248.7378\u001b[0m  0.0095\n",
      "      3      \u001b[36m238.8357\u001b[0m  0.0090\n",
      "      4      \u001b[36m229.3974\u001b[0m  0.0116\n",
      "      5      \u001b[36m220.1867\u001b[0m  0.0101\n",
      "      6      \u001b[36m211.1628\u001b[0m  0.0088\n",
      "      7      \u001b[36m202.7144\u001b[0m  0.0108\n",
      "      8      \u001b[36m194.7346\u001b[0m  0.0106\n",
      "      9      \u001b[36m186.6107\u001b[0m  0.0104\n",
      "     10      \u001b[36m179.1036\u001b[0m  0.0108\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m290.1546\u001b[0m  0.0069\n",
      "      2      \u001b[36m279.5425\u001b[0m  0.0112\n",
      "      3      \u001b[36m269.0094\u001b[0m  0.0098\n",
      "      4      \u001b[36m258.9151\u001b[0m  0.0087\n",
      "      5      \u001b[36m249.2876\u001b[0m  0.0112\n",
      "      6      \u001b[36m240.2287\u001b[0m  0.0103\n",
      "      7      \u001b[36m231.0015\u001b[0m  0.0083\n",
      "      8      \u001b[36m222.2918\u001b[0m  0.0104\n",
      "      9      \u001b[36m213.8312\u001b[0m  0.0096\n",
      "     10      \u001b[36m205.7331\u001b[0m  0.0108\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.5339\u001b[0m  0.0098\n",
      "      2      \u001b[36m259.6686\u001b[0m  0.0097\n",
      "      3      \u001b[36m250.0380\u001b[0m  0.0205\n",
      "      4      \u001b[36m240.5686\u001b[0m  0.0094\n",
      "      5      \u001b[36m231.5679\u001b[0m  0.0095\n",
      "      6      \u001b[36m222.6004\u001b[0m  0.0112\n",
      "      7      \u001b[36m214.4749\u001b[0m  0.0138\n",
      "      8      \u001b[36m206.0607\u001b[0m  0.0133\n",
      "      9      \u001b[36m198.1841\u001b[0m  0.0128\n",
      "     10      \u001b[36m190.4992\u001b[0m  0.0079\n",
      "     11      \u001b[36m182.8404\u001b[0m  0.0132\n",
      "     12      \u001b[36m176.0595\u001b[0m  0.0115\n",
      "     13      \u001b[36m169.0650\u001b[0m  0.0095\n",
      "     14      \u001b[36m162.4372\u001b[0m  0.0085\n",
      "     15      \u001b[36m156.0191\u001b[0m  0.0136\n",
      "     16      \u001b[36m149.3099\u001b[0m  0.0109\n",
      "     17      \u001b[36m143.2214\u001b[0m  0.0099\n",
      "     18      \u001b[36m137.5804\u001b[0m  0.0096\n",
      "     19      \u001b[36m132.0234\u001b[0m  0.0118\n",
      "     20      \u001b[36m126.6372\u001b[0m  0.0110\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m258.9746\u001b[0m  0.0078\n",
      "      2      \u001b[36m248.3134\u001b[0m  0.0106\n",
      "      3      \u001b[36m238.7531\u001b[0m  0.0350\n",
      "      4      \u001b[36m229.3138\u001b[0m  0.0192\n",
      "      5      \u001b[36m219.8859\u001b[0m  0.0300\n",
      "      6      \u001b[36m211.3632\u001b[0m  0.0276\n",
      "      7      \u001b[36m202.6284\u001b[0m  0.0193\n",
      "      8      \u001b[36m194.5691\u001b[0m  0.0233\n",
      "      9      \u001b[36m186.7880\u001b[0m  0.0164\n",
      "     10      \u001b[36m178.8854\u001b[0m  0.0149\n",
      "     11      \u001b[36m171.6668\u001b[0m  0.0125\n",
      "     12      \u001b[36m164.7436\u001b[0m  0.0164\n",
      "     13      \u001b[36m157.8625\u001b[0m  0.0161\n",
      "     14      \u001b[36m151.1978\u001b[0m  0.0135\n",
      "     15      \u001b[36m144.6948\u001b[0m  0.0141\n",
      "     16      \u001b[36m138.5591\u001b[0m  0.0140\n",
      "     17      \u001b[36m132.8564\u001b[0m  0.0180\n",
      "     18      \u001b[36m126.8665\u001b[0m  0.0122\n",
      "     19      \u001b[36m121.3068\u001b[0m  0.0146\n",
      "     20      \u001b[36m116.2031\u001b[0m  0.0138\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m290.1220\u001b[0m  0.0161\n",
      "      2      \u001b[36m279.4883\u001b[0m  0.0096\n",
      "      3      \u001b[36m269.1204\u001b[0m  0.0128\n",
      "      4      \u001b[36m259.1737\u001b[0m  0.0153\n",
      "      5      \u001b[36m249.4437\u001b[0m  0.0153\n",
      "      6      \u001b[36m240.0352\u001b[0m  0.0141\n",
      "      7      \u001b[36m231.0680\u001b[0m  0.0132\n",
      "      8      \u001b[36m222.1501\u001b[0m  0.0141\n",
      "      9      \u001b[36m213.7833\u001b[0m  0.0204\n",
      "     10      \u001b[36m205.6292\u001b[0m  0.0149\n",
      "     11      \u001b[36m197.8108\u001b[0m  0.0166\n",
      "     12      \u001b[36m190.3455\u001b[0m  0.0160\n",
      "     13      \u001b[36m182.7396\u001b[0m  0.0147\n",
      "     14      \u001b[36m175.7365\u001b[0m  0.0149\n",
      "     15      \u001b[36m168.8479\u001b[0m  0.0162\n",
      "     16      \u001b[36m162.2130\u001b[0m  0.0156\n",
      "     17      \u001b[36m155.6425\u001b[0m  0.0184\n",
      "     18      \u001b[36m149.4526\u001b[0m  0.0184\n",
      "     19      \u001b[36m143.1846\u001b[0m  0.0189\n",
      "     20      \u001b[36m137.4847\u001b[0m  0.0125\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.4837\u001b[0m  0.0164\n",
      "      2      \u001b[36m249.8948\u001b[0m  0.0226\n",
      "      3      \u001b[36m231.2854\u001b[0m  0.0254\n",
      "      4      \u001b[36m214.2689\u001b[0m  0.0218\n",
      "      5      \u001b[36m198.0210\u001b[0m  0.0164\n",
      "      6      \u001b[36m182.9646\u001b[0m  0.0138\n",
      "      7      \u001b[36m168.6386\u001b[0m  0.0211\n",
      "      8      \u001b[36m155.2865\u001b[0m  0.0194\n",
      "      9      \u001b[36m142.9915\u001b[0m  0.0246\n",
      "     10      \u001b[36m131.1320\u001b[0m  0.0353\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m258.8188\u001b[0m  0.0422\n",
      "      2      \u001b[36m238.9205\u001b[0m  0.0243\n",
      "      3      \u001b[36m220.0130\u001b[0m  0.0389\n",
      "      4      \u001b[36m202.6742\u001b[0m  0.0291\n",
      "      5      \u001b[36m186.5364\u001b[0m  0.0409\n",
      "      6      \u001b[36m171.4368\u001b[0m  0.0313\n",
      "      7      \u001b[36m157.3131\u001b[0m  0.0299\n",
      "      8      \u001b[36m144.1268\u001b[0m  0.0130\n",
      "      9      \u001b[36m131.8259\u001b[0m  0.0179\n",
      "     10      \u001b[36m120.5963\u001b[0m  0.0183\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m290.1211\u001b[0m  0.0173\n",
      "      2      \u001b[36m269.1937\u001b[0m  0.0081\n",
      "      3      \u001b[36m249.6568\u001b[0m  0.0190\n",
      "      4      \u001b[36m230.9914\u001b[0m  0.0211\n",
      "      5      \u001b[36m213.7627\u001b[0m  0.0183\n",
      "      6      \u001b[36m197.6057\u001b[0m  0.0248\n",
      "      7      \u001b[36m182.4439\u001b[0m  0.0202\n",
      "      8      \u001b[36m168.3255\u001b[0m  0.0222\n",
      "      9      \u001b[36m155.0838\u001b[0m  0.0188\n",
      "     10      \u001b[36m142.7392\u001b[0m  0.0183\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m269.5103\u001b[0m  0.0163\n",
      "      2      \u001b[36m249.9383\u001b[0m  0.0121\n",
      "      3      \u001b[36m231.4792\u001b[0m  0.0189\n",
      "      4      \u001b[36m214.1984\u001b[0m  0.0166\n",
      "      5      \u001b[36m198.0716\u001b[0m  0.0205\n",
      "      6      \u001b[36m182.9909\u001b[0m  0.0186\n",
      "      7      \u001b[36m168.6881\u001b[0m  0.0171\n",
      "      8      \u001b[36m155.2703\u001b[0m  0.0170\n",
      "      9      \u001b[36m142.8466\u001b[0m  0.0326\n",
      "     10      \u001b[36m131.1142\u001b[0m  0.0187\n",
      "     11      \u001b[36m120.1955\u001b[0m  0.0163\n",
      "     12      \u001b[36m110.0936\u001b[0m  0.0166\n",
      "     13      \u001b[36m100.7176\u001b[0m  0.0183\n",
      "     14       \u001b[36m92.0213\u001b[0m  0.0187\n",
      "     15       \u001b[36m83.8707\u001b[0m  0.0148\n",
      "     16       \u001b[36m76.3765\u001b[0m  0.0256\n",
      "     17       \u001b[36m69.7720\u001b[0m  0.0172\n",
      "     18       \u001b[36m63.3497\u001b[0m  0.0190\n",
      "     19       \u001b[36m57.5025\u001b[0m  0.0315\n",
      "     20       \u001b[36m52.3212\u001b[0m  0.0123\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m258.9063\u001b[0m  0.0178\n",
      "      2      \u001b[36m239.0449\u001b[0m  0.0227\n",
      "      3      \u001b[36m220.2330\u001b[0m  0.0195\n",
      "      4      \u001b[36m202.8276\u001b[0m  0.0201\n",
      "      5      \u001b[36m186.5124\u001b[0m  0.0243\n",
      "      6      \u001b[36m171.4477\u001b[0m  0.0151\n",
      "      7      \u001b[36m157.5669\u001b[0m  0.0130\n",
      "      8      \u001b[36m144.1862\u001b[0m  0.0155\n",
      "      9      \u001b[36m131.9112\u001b[0m  0.0212\n",
      "     10      \u001b[36m120.5116\u001b[0m  0.0185\n",
      "     11      \u001b[36m109.8410\u001b[0m  0.0181\n",
      "     12       \u001b[36m99.9468\u001b[0m  0.0134\n",
      "     13       \u001b[36m90.7863\u001b[0m  0.0195\n",
      "     14       \u001b[36m82.1947\u001b[0m  0.0195\n",
      "     15       \u001b[36m74.7130\u001b[0m  0.0119\n",
      "     16       \u001b[36m67.3926\u001b[0m  0.0114\n",
      "     17       \u001b[36m61.0730\u001b[0m  0.0159\n",
      "     18       \u001b[36m55.2886\u001b[0m  0.0165\n",
      "     19       \u001b[36m49.8244\u001b[0m  0.0173\n",
      "     20       \u001b[36m45.3397\u001b[0m  0.0140\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1      \u001b[36m289.8438\u001b[0m  0.0176\n",
      "      2      \u001b[36m269.1639\u001b[0m  0.0309\n",
      "      3      \u001b[36m249.6682\u001b[0m  0.0244\n",
      "      4      \u001b[36m231.0816\u001b[0m  0.0228\n",
      "      5      \u001b[36m213.7665\u001b[0m  0.0171\n",
      "      6      \u001b[36m197.7283\u001b[0m  0.0172\n",
      "      7      \u001b[36m182.5226\u001b[0m  0.0214\n",
      "      8      \u001b[36m168.3531\u001b[0m  0.0193\n",
      "      9      \u001b[36m155.1107\u001b[0m  0.0207\n",
      "     10      \u001b[36m142.5789\u001b[0m  0.0227\n",
      "     11      \u001b[36m130.9308\u001b[0m  0.0182\n",
      "     12      \u001b[36m119.8435\u001b[0m  0.0181\n",
      "     13      \u001b[36m109.6230\u001b[0m  0.0146\n",
      "     14       \u001b[36m99.9607\u001b[0m  0.0199\n",
      "     15       \u001b[36m91.1408\u001b[0m  0.0211\n",
      "     16       \u001b[36m83.1261\u001b[0m  0.0176\n",
      "     17       \u001b[36m75.5029\u001b[0m  0.0178\n",
      "     18       \u001b[36m68.7297\u001b[0m  0.0141\n",
      "     19       \u001b[36m62.3495\u001b[0m  0.0190\n",
      "     20       \u001b[36m56.4876\u001b[0m  0.0171\n",
      "\n",
      " gs.best_score_ = 0.4, gs.best_params = {'lr': 0.01, 'max_epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'lr': [0.01, 0.02],\n",
    "    'max_epochs': [10, 20],\n",
    "}\n",
    "\n",
    "GPWrapper = VariationalGaussianProcessClassifier(\n",
    "    module = GPClassificationModel,\n",
    "    train_split = None,\n",
    ")\n",
    "\n",
    "gs = GridSearchCV(GPWrapper, params, refit=False, cv=3, scoring='accuracy', \n",
    "                  return_train_score=False)  #  Use a different scoring function maybe?\n",
    "\n",
    "gs.fit(X=train_x, y=train_y)\n",
    "print('\\n gs.best_score_ = {}, gs.best_params = {}'.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Need to comment out **line 157 - 161** of `.../anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/__init__.py`\n",
    "```\n",
    "if hasattr(X, 'take') and (hasattr(indices, 'dtype') and\n",
    "                           indices.dtype.kind == 'i'):\n",
    "    # This is often substantially faster than X[indices]\n",
    "    return X.take(indices, axis=0)\n",
    "else:\n",
    "```\n",
    "Otherwise an error would occur saying\n",
    "`argument 'index' (position 1) must be Tensor, not numpy.ndarray`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
